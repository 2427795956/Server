## 问题和测试

1. 目前发现的能够改进的问题

   - TCPbuffer采用string造成的效率问题
   - TCP关闭时可能不够优雅(read到0时，有可能客户端只关闭了写端，此时应该把该发送的发完，再关闭)
   - 发送文件时，可以采用sendfile方式提高效率
   - 给日志增加级别

2. 什么时候需要用锁？
   多个线程需要访问共有变量的时候需要加锁，项目中均采用RAII设计的互斥锁进行临界区的保护。
   锁的争用只在主线程和IO线程中，具体在几个地方：需要更改跨线程回调函数列表时，等待子线程创建时，日志前后端访问缓冲区时。

3. 注册回调方式？

   最开始的回调是采用std::bind的方式进行注册的，还可以通过lambda表达式进行注册，和bind方式相比，有几个优点：1. 不需要占位符，绑定参数，可以清楚的看出参数之间的关系，看起来更加直观 2. lambda使用的是常规的函数调用形式，编译器可能会进行内联优化，而bind是通过函数指针的形式，编译器不太会优化这种类型的调用，所以lambda的效率可能会更高。

4. 测试

   测试环境：OS: Ubuntu 18.04 内存: 8g CPU: i7-4510U(双核四线程)

   线程池线程个数为3

   本来计划采用webbench，但是不支持长连接，改用wrk进行测试，以下为测试结果

   - 空闲负载，空闲时，各个线程CPU为0%

   ![](./assets/空闲.png)

   - 短连接测试 短连接qps为26704

     ![](./assets/短连接测试.png)

   - 长连接测试 长连接qps为93981，长链接qps与短连接之比为3.52

     ![](./assets/长连接测试.png)

   - 短连接负载，各个线程的负载较为均衡

     ![](./assets/短连接负载.png)

   - 长连接负载 主线程几乎没有负载，线程池的负载较高

     ![](./assets/长连接负载.png)

   结果分析：首先空闲时基本没有负载，因为没有任何任务需要处理。从长短链接的测试可以看出，长连接处理的qps大约为短连接的3-4倍，造成这种巨大差异主要是因为长连接没有重新建立链接的开销，不需要频繁的accept/shutdown/close等系统调用，并且不需要频繁销毁/建立Connection类实例。从负载方面来看，短连接时，因为MainReactor需要频繁的建立新的链接，因此各个线程间较为均衡。长连接时，因为没有新的链接需要处理，所以MainReactor负载为0。

   在进行测试时发现bug，不加定时器（短连接），Connection正常析构，加了定时器，因为超时关闭的函数持有Connection，同时Connection又通过HttpINfomation持有定时器，导致均不会被析构。因为每个Http链接需要通过定时器来进行关闭，但是关闭有需要调用Connection类内的函数，为了解决循环引用的问题，我们将HttpInfomation类内的Timer改为weak_ptr，因为weak_ptr不影响shared_ptr的计数，这样，可以值得Timer在TimerManager里被释放时正常被析构，也使得Connection类得以被析构。