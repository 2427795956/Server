# 版本历史
每形成一个阶段性版本就在此进行一下总结，回顾一下整体架构，思考未来改进的方向

## 0.1
看了muduo，同时参考了网上其他人写的，根据自己的理解，以及muduo源码，编写了一个web服务器
![0.1框架](./assets/0.1框架.png)
整体框架如图，主reactor负责接收新的连接，建立连接后分配给某个SubReactor。跨线程调用利用回调函数队列实现，需要加锁，锁的竞争只在MainReactor和某一SubReactor之间，不同SubReactor之间没有联系。

### Key Points
* 使用epoll水平触发+非阻塞IO，使用reactor模式
* 采用one loop per thread结构，使用多线程充分利用多核CPU，并使用线程池避免创建销毁的开销
* 使用eventfd实现了线程的异步唤醒
* 使用基于map的定时器，主要用于HTTP超时关闭，使用timerfd唤醒等待线程
* 使用双缓冲技术，实现了简单的异步日志
* 使用了RAII技术设计了多个类
* 解析了HTTP请求(GET HEAD)，支持长短连接

### 问题及测试：

1. accept的个数
    三种策略：accept一个，accept N个，accept直至没有连接到达。长短连接适合的方式？

  目前采用了accept一个的策略，似乎除了可能多判断一次以外，没有什么坏处？

2. 其他唤醒方式
    使用pipe进行唤醒，让IO线程监控管道是否可读

3. 什么时候需要用锁？
    多个线程需要访问共有变量的时候需要加锁，项目中均采用RAII设计的互斥锁进行临界区的保护。
    锁的争用只在主线程和IO线程中，具体在几个地方：需要更改跨线程回调函数列表时，等待子线程创建时，日志前后端访问缓冲区时

4. epoll和poll以及select的对比？为什么高效？

5. 将磁盘文件作为包体发送？

    有几种方式，逐个分析：

    * 调用read读入用户buffer，然后再调用write写入socket

      这种方式数据传输流程为：磁盘 >> 内核buffer >> 用户buffer >> 内核buffer(和上个不同，和socket关联的) >> 网卡设备。可以看到这种方案调用了2次系统调用，产生了4次上下文切换，同时，进行了4次数据拷贝，这些都会造成性能的降低。

    * 利用mmap进行映射，在write写入socket

      这种方式数据流程为：磁盘 >> 内核buffer(用户和内核共享) >> 内核buffer(和socket关联)>>网卡设备。可以看到，使用mmap可以减少内核buffer和用户buffer之间的操作，只需要3次数据拷贝

    * 利用sendfile系统调用

      这种方式数据流程为：磁盘 >> 内核buffer >> 内核buffer(和socket关联，只记录内核buffer里的位置和偏移量) >> 网卡。

      使用sendfile只需要2次拷贝，并且只调用了一次系统调用，也就是2次上下文切换。

    目前采用的是mmap方式，主要是因为我们需要在http正文前添加状态行以及消息报头等内容，而sendfile方式不方便进行添加，后续可以尝试更改为先发送状态行和报头，再调用sendfile。

6. 定时器设计

   TimerManager类需要选择合适的结构进行组织未到期的timer，并且能够快速的找到timer，一般有几种方式：

   * 使用按到期时间排序的线性表

     一般采用线性查找，复杂度为O(N)

   * 使用堆进行组织

     插入等常用操作降为O(logN)，但是priority_queue删除某一timer不方便，可以将其设置为delte标志跳过运行超时

   * 使用map(红黑树)

     复杂度仍为O(logN)，但是使用到期时间作为key，如果遇到同样的到期时间的话，会产生冲突，可以通过使用multimap或者区分key解决

     采用堆方式，不方便实时删除，会导致持有的timer被推迟释放，由于链接的超时回调延长了Connection类的生命周期，导致Connection类被推迟释放，造成不必要的内存占用。

     因此，采用multimap进行定时器的管理，可以使删除和插入均有较好的性能。

7. 版本0.1测试

   测试环境：OS: Ubuntu 18.04 内存: 8g CPU: i7-4510U(双核四线程)

   线程池线程个数为3

   本来计划采用webbench，但是不支持长连接，改用wrk进行测试，以下为测试结果

   * 空闲负载，空闲时，各个线程CPU为0%

   ![](./assets/空闲.png)

   * 短连接测试 短连接qps为30564

     ![](./assets/短连接测试.png)

   * 长连接测试 长连接qps为105801，长链接qps与短连接之比为3.46

     ![](./assets/长连接测试.png)

   * 短连接负载，各个线程的负载较为均衡

     ![](./assets/短连接负载.png)

   * 长连接负载 主线程几乎没有负载，线程池的负载较高

     ![](./assets/长连接负载.png)

   结果分析：首先空闲时基本没有负载，因为没有任何任务需要处理。从长短链接的测试可以看出，长连接处理的qps大约为短连接的3-4倍，造成这种巨大差异主要是因为长连接没有重新建立链接的开销，不需要频繁的accept/shutdown/close等系统调用，并且不需要频繁销毁/建立Connection类实例。从负载方面来看，短连接时，因为MainReactor需要频繁的建立新的链接，因此各个线程间较为均衡。长连接时，因为没有新的链接需要处理，所以MainReactor负载为0。

   在进行测试时发现bug，不加定时器（短连接），Connection正常析构，加了定时器，因为超时关闭的函数持有Connection，同时Connection又通过HttpINfomation持有定时器，导致均不会被析构。因为每个Http链接需要通过定时器来进行关闭，但是关闭有需要调用Connection类内的函数，为了解决循环引用的问题，我们将HttpInfomation类内的Timer改为weak_ptr，因为weak_ptr不影响shared_ptr的计数，这样，可以值得Timer在TimerManager里被释放时正常被析构，也使得Connection类得以被析构。

   至此，版本0.1总结暂时结束，接下来花点时间阅读《深入理解Nginx》，看看有没有能够借鉴的地方。

   使用Nginx的echo模块进行简单测试，由于Nginx输出了更多的内容，这里只是对qps进行一个简单的对比，开启4个进程：短连接36658，长连接104133，可以看到qps和我的服务器差别不大，可以说明服务器目前版本没有较大的问题。

   

   